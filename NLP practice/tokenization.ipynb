{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Hello Welcome, to Krish Naik's NLP tutorials. \n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to Krish Naik's NLP tutorials. \n",
      "Please do watch the entire course! to become expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizati0on \n",
    "# sentences ---> paragraphs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph --> words\n",
    "# Sentence -->words\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
      "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordpunct tokenization\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we want to tokenize the words and the fullstop get tokenize at the last only not at the previous fullstops\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and its types-Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"congratulations\") #  disadv. of stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RegexpStemmer class\n",
    ">> NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eats--->eat\n",
      "eaten--->eaten\n",
      "writing--->write\n",
      "writes--->write\n",
      "programming--->program\n",
      "programs--->program\n",
      "history--->histori\n",
      "finally--->final\n",
      "finalized--->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"--->\"+stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\"), stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"fairly\"), stemmer.stem(\"sportingly\"),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer\n",
    "\n",
    ">> Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
    "\n",
    ">> NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('going', 'go')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# :param pos: The Part Of Speech tag. Valid options are \"n\" for nouns,\n",
    "#     \"v\" for verbs, \"a\" for adjectives, \"r\" for adverbs and \"s\" for satellite adjectives.\n",
    "lemmatizer.lemmatize(\"going\"), lemmatizer.lemmatize(\"going\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eats--->eat\n",
      "eaten--->eat\n",
      "writing--->write\n",
      "writes--->write\n",
      "programming--->program\n",
      "programs--->program\n",
      "history--->history\n",
      "finally--->finally\n",
      "finalized--->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"--->\"+lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
       " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
       " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'My second vision for India’s development.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
       " 'We have a 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isn’t this incorrect?',\n",
       " 'I have a third vision.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr. Vikram Sarabhai of the Dept.',\n",
       " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I see four milestones in my career']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = str.lower(sentences[i])\n",
    "    words = (nltk.word_tokenize(sentences[i]))\n",
    "    words = [stemmer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))] # take words only which is not in stopwords\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year history , people world come invaded u , captured land , conquered mind .',\n",
       " 'alexander onwards , greek , turk , mogul , portuguese , british , french , dutch , came looted u , took .',\n",
       " 'yet done nation .',\n",
       " 'conquered anyone .',\n",
       " 'grabbed land , culture , history tried enforce way life .',\n",
       " '?',\n",
       " 'respect freedom others.that first vision freedom .',\n",
       " 'believe india got first vision 1857 , started war independence .',\n",
       " 'freedom must protect nurture build .',\n",
       " 'free , one respect u .',\n",
       " 'second vision india ’ development .',\n",
       " 'fifty year developing nation .',\n",
       " 'time see developed nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverty level falling .',\n",
       " 'achievement globally recognised today .',\n",
       " 'yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'believe unless india stand world , one respect u .',\n",
       " 'strength respect strength .',\n",
       " 'must strong military power also economic power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortune worked three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeeded dr. brahm prakash , father nuclear material .',\n",
       " 'lucky worked three closely consider great opportunity life .',\n",
       " 'see four milestone career']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging\n",
    "\n",
    ">> Basically it tags the word according to their nouns, adjective, verbs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('three', 'CD'), ('vision', 'NN'), ('india', 'NN'), ('.', '.')]\n",
      "[('3000', 'CD'), ('year', 'NN'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('u', 'JJ'), (',', ','), ('captured', 'JJ'), ('land', 'NN'), (',', ','), ('conquered', 'VBD'), ('mind', 'NN'), ('.', '.')]\n",
      "[('alexander', 'NN'), ('onwards', 'NNS'), (',', ','), ('greek', 'NN'), (',', ','), ('turk', 'NN'), (',', ','), ('mogul', 'NN'), (',', ','), ('portuguese', 'JJ'), (',', ','), ('british', 'JJ'), (',', ','), ('french', 'JJ'), (',', ','), ('dutch', 'VB'), (',', ','), ('came', 'VBD'), ('looted', 'VBN'), ('u', 'NN'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
      "[('yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "[('conquered', 'VBN'), ('anyone', 'NN'), ('.', '.')]\n",
      "[('grabbed', 'JJ'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('?', '.')]\n",
      "[('respect', 'NN'), ('freedom', 'NN'), ('others.that', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('believe', 'VB'), ('india', 'NN'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('war', 'NN'), ('independence', 'NN'), ('.', '.')]\n",
      "[('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
      "[('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('u', 'NN'), ('.', '.')]\n",
      "[('second', 'JJ'), ('vision', 'NN'), ('india', 'NN'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
      "[('fifty', 'JJ'), ('year', 'NN'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
      "[('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
      "[('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nation', 'NN'), ('world', 'NN'), ('term', 'NN'), ('gdp', 'NN'), ('.', '.')]\n",
      "[('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('area', 'NN'), ('.', '.')]\n",
      "[('poverty', 'NN'), ('level', 'NN'), ('falling', 'VBG'), ('.', '.')]\n",
      "[('achievement', 'NN'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
      "[('yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
      "[('’', 'NN'), ('incorrect', 'NN'), ('?', '.')]\n",
      "[('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
      "[('india', 'NN'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
      "[('believe', 'VB'), ('unless', 'IN'), ('india', 'JJ'), ('stand', 'NN'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('u', 'NN'), ('.', '.')]\n",
      "[('strength', 'NN'), ('respect', 'NN'), ('strength', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
      "[('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('mind', 'NN'), ('.', '.')]\n",
      "[('dr.', 'NN'), ('vikram', 'NN'), ('sarabhai', 'NN'), ('dept', 'NN'), ('.', '.')]\n",
      "[('space', 'NN'), (',', ','), ('professor', 'NN'), ('satish', 'JJ'), ('dhawan', 'NN'), (',', ','), ('succeeded', 'VBD'), ('dr.', 'NN'), ('brahm', 'NN'), ('prakash', 'NN'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
      "[('lucky', 'JJ'), ('worked', 'VBD'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('see', 'VB'), ('four', 'CD'), ('milestone', 'NN'), ('career', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = str.lower(sentences[i])\n",
    "    words = (nltk.word_tokenize(sentences[i]))\n",
    "    words = [stemmer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))] # take words only which is not in stopwords\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Monument', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(nltk.pos_tag(nltk.word_tokenize(\"Taj Mahal is a beautiful Monument\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_element = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgling in c:\\users\\ayush\\desktop\\study\\big oh training\\big oh training\\big-oh-training\\.venv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: svgwrite in c:\\users\\ayush\\desktop\\study\\big oh training\\big oh training\\big-oh-training\\.venv\\lib\\site-packages (from svgling) (1.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1280.0,168.0\" width=\"1280px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"3.125%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.5625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.375%\" x=\"3.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tower</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.125%\" x=\"12.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.0625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"15.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">built</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"20%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.875%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"23.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1887</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"27.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.75%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"30%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1889</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.875%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"33.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.625%\" x=\"36.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Gustave</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.5625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.875%\" x=\"46.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"48.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">whose</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50.9375%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.625%\" x=\"53.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.9375%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.125%\" x=\"58.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">specialized</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"66.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.25%\" x=\"69.375%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">building</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.5%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"75.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">metal</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"80%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">frameworks</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.75%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.125%\" x=\"87.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.0625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"90.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">structures</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.375%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.875%\" x=\"98.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.0625%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Eiffel', 'NNP'), ('Tower', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('from', 'IN'), ('1887', 'CD'), ('to', 'TO'), ('1889', 'CD'), ('by', 'IN'), Tree('PERSON', [('Gustave', 'NNP'), ('Eiffel', 'NNP')]), (',', ','), ('whose', 'WP$'), ('company', 'NN'), ('specialized', 'VBD'), ('in', 'IN'), ('building', 'NN'), ('metal', 'NN'), ('frameworks', 'NNS'), ('and', 'CC'), ('structures', 'NNS'), ('.', '.')])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(tag_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ayush\\desktop\\study\\big oh training\\big oh training\\big-oh-training\\.venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ayush\\desktop\\study\\big oh training\\big oh training\\big-oh-training\\.venv\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.0 MB 14.9 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.6/24.0 MB 7.7 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.8/24.0 MB 6.1 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.0/24.0 MB 5.6 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.2/24.0 MB 5.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.3/24.0 MB 4.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.5/24.0 MB 4.8 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.6/24.0 MB 4.6 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.0 MB 4.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.0/24.0 MB 4.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.1/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.5/24.0 MB 2.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 3.5/24.0 MB 3.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.7/24.0 MB 3.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.1/24.0 MB 3.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 4.3/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 4.5/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 4.7/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 4.8/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.0/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.2/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.3/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.5/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.7/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.8/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.0/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.2/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 6.4/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 6.5/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 6.7/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 6.8/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 7.0/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.2/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.4/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.5/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.7/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 7.9/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.1/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.2/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.4/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 8.6/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 8.7/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 8.9/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.0/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.2/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.4/24.0 MB 3.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.6/24.0 MB 3.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 9.8/24.0 MB 3.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 9.9/24.0 MB 3.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 10.1/24.0 MB 3.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.3/24.0 MB 3.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.5/24.0 MB 3.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.6/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.8/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 10.9/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.1/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.3/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.3/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.4/24.0 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 11.7/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 11.9/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.1/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.3/24.0 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.5/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.7/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.8/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.0/24.0 MB 4.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.2/24.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.3/24.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.5/24.0 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.7/24.0 MB 3.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 13.8/24.0 MB 3.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.0/24.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.2/24.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.4/24.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.5/24.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.7/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.9/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.0/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.2/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.5/24.0 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 15.7/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 15.9/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.1/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.2/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.4/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.6/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.7/24.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 16.9/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.1/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.3/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.4/24.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.6/24.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.8/24.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.9/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.1/24.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.3/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.4/24.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.6/24.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 18.8/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.0/24.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.1/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.3/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.5/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.6/24.0 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.8/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.0/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.1/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.3/24.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 20.5/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.6/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.8/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.1/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.3/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.5/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.7/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.8/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.2/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.4/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.5/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.7/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 22.9/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.2/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.4/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.5/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.7/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 51.2/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB 498.0 kB/s eta 0:00:00\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec, keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 19.5% 324.3/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "vec_king = wv[\"king\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
